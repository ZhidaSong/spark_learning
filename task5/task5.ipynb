{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:\\Spark\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"Simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row,functions\n",
    "from pyspark.ml.linalg import Vector,Vectors\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer,HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression,LogisticRegressionModel,BinaryLogisticRegressionSummary, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    rel = {}\n",
    "    rel['features'] = Vectors.dense(float(x[0]),float(x[1]),float(x[2]),float(x[3]))\n",
    "    rel['label'] = str(x[4])\n",
    "    return rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =sc.textFile(\"iris.txt\").map(lambda line: line.split(',')).map(lambda p: Row(**f(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"iris\")\n",
    "df = spark.sql(\"select * from iris where label != 'Iris-setosa'\")\n",
    "rel = df.map(lambda t : str(t[1])+\":\"+str(t[0])).collect()\n",
    "for item in rel:\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(df)\n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").fit(df)\n",
    "featureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = vecIdx_53b988077b38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData = df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPipeline =  Pipeline().setStages([labelIndexer, featureIndexer, lr, labelConverter])\n",
    "lrPipelineModel = lrPipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPredictions = lrPipelineModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preRel = lrPredictions.select(\"predictedLabel\", \"label\", \"features\", \"probability\").collect()\n",
    "for item in preRel:\n",
    "    print(str(item['label'])+','+str(item['features'])+'-->prob='+str(item['probability'])+',predictedLabel'+str(item['predictedLabel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\")\n",
    "lrAccuracy = evaluator.evaluate(lrPredictions)\n",
    "print(\"Test Error = \" + str(1.0 - lrAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lrPipelineModel.stages[2]\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients)+\"Intercept: \"+str(lrModel.intercept)+\"numClasses: \"+str(lrModel.numClasses)+\"numFeatures: \"+str(lrModel.numFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "for item in objectiveHistory:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainingSummary.areaUnderROC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fMeasure = trainningSummary.fMeasureByThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxFMeasure = fMeasure.select(functions.max(\"F-Measure\")).head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestThreshold = fMeasure.where(fMeasure[\"F-Measure\"]== maxFMeasure).select(\"threshold\").head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr =  LogisticRegression().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8).setFamily(\"multinomial\")\n",
    " \n",
    "mlrPipeline = Pipeline().setStages([labelIndexer, featureIndexer, mlr, labelConverter])\n",
    " \n",
    "mlrPipelineModel = mlrPipeline.fit(trainingData)\n",
    " \n",
    "mlrPreRel = mlrPredictions.select(\"predictedLabel\", \"label\", \"features\", \"probability\").collect()\n",
    "for item in mlrPreRel:\n",
    "    print('('+str(item['label'])+','+str(item['features'])+')-->prob='+str(item['probability'])+',predictLabel='+str(item['predictedLabel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlrAccuracy = evaluator.evaluate(mlrPredictions)\n",
    " \n",
    "print(\"Test Error = \" + str(1.0 - mlrAccuracy))\n",
    " \n",
    "Test Error = 0.48730158730158735\n",
    " \n",
    " \n",
    "mlrModel = mlrPipelineModel.stages[2]\n",
    " \n",
    "print(\"Multinomial coefficients: \" +str(mlrModel.coefficientMatrix)+\"Multin\n",
    "omial intercepts: \"+str(mlrModel.interceptVector)+\"numClasses: \"+str(mlrModel.numClasses)+\n",
    "\"numFeatures: \"+str(mlrModel.numFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlrPreRel = mlrPredictions.select(\"predictedLabel\", \"label\", \"features\", \"probability\").collect()\n",
    "for item in mlrPreRel:\n",
    "    print('('+str(item['label'])+','+str(item['features'])+')-->prob='+str(item['probability'])+',predictLabel='+str(item['predictedLabel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector,Vectors\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer,VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    rel = {}\n",
    "    rel['features'] = Vectors.dense(float(x[0]),float(x[1]),float(x[2]),float(x[3]))\n",
    "    rel['label'] = str(x[4])\n",
    "    return rel\n",
    " \n",
    "data = sc.textFile(\"file:///usr/local/spark/iris.txt\").map(lambda line: line.split(',')).map(lambda p: Row(**f(p))).toDF()\n",
    " \n",
    "data.createOrReplaceTempView(\"iris\")\n",
    " \n",
    "df = spark.sql(\"select * from iris\")\n",
    " \n",
    "rel = df.rdd.map(lambda t : str(t[1])+\":\"+str(t[0])).collect()\n",
    "for item in rel:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别获取标签列和特征列，进行索引，并进行了重命名。\n",
    "labelIndexer = StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(df)\n",
    " \n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").setMaxCategories(4).fit(df)\n",
    " \n",
    "# 这里我们设置一个labelConverter，目的是把预测的类别重新转化成字符型的。\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n",
    "# 接下来，我们把数据集随机分成训练集和测试集，其中训练集占70%。\n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需要的包\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel,DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# 训练决策树模型,这里我们可以通过setter的方法来设置决策树的参数，也可以用ParamMap来设置（具体的可以查看spark mllib的官网）。具体的可以设置的参数可以通过explainParams()来获取。\n",
    "dtClassifier = DecisionTreeClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\")\n",
    "# 在pipeline中进行设置\n",
    "pipelinedClassifier = Pipeline().setStages([labelIndexer, featureIndexer, dtClassifier, labelConverter])\n",
    "# 训练决策树模型\n",
    "modelClassifier = pipelinedClassifier.fit(trainingData)\n",
    "# 进行预测\n",
    "predictionsClassifier = modelClassifier.transform(testData)\n",
    "# 查看部分预测的结果\n",
    "predictionsClassifier.select(\"predictedLabel\", \"label\", \"features\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatorClassifier = MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    " \n",
    "accuracy = evaluatorClassifier.evaluate(predictionsClassifier)\n",
    " \n",
    "print(\"Test Error = \" + str(1.0 - accuracy))\n",
    "Test Error = 0.05882352941176472\n",
    " \n",
    "treeModelClassifier = modelClassifier.stages[2]\n",
    " \n",
    "print(\"Learned classification tree model:\\n\" + str(treeModelClassifier.toDebugString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需要的包\n",
    "from pyspark.ml.regression import DecisionTreeRegressionModel,DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# 训练决策树模型\n",
    "dtRegressor = DecisionTreeRegressor().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\")\n",
    "# 在pipeline中进行设置\n",
    "pipelineRegressor = Pipeline().setStages([labelIndexer, featureIndexer, dtRegressor, labelConverter])\n",
    "# 训练决策树模型\n",
    "modelRegressor = pipelineRegressor.fit(trainingData)\n",
    "# 进行预测\n",
    "predictionsRegressor = modelRegressor.transform(testData)\n",
    "# 查看部分预测结果\n",
    "predictionsRegressor.select(\"predictedLabel\", \"label\", \"features\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluatorRegressor = RegressionEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"rmse\")\n",
    " \n",
    "rmse = evaluatorRegressor.evaluate(predictionsRegressor)\n",
    " \n",
    "print(\"Root Mean Squared Error (RMSE) on test data = \" +str(rmse))\n",
    "Root Mean Squared Error (RMSE) on test data = 0.24253562503633297\n",
    " \n",
    "treeModelRegressor = modelRegressor.stages[2]\n",
    " \n",
    "print(\"Learned regression tree model:\\n\" + str(treeModelRegressor.toDebugString))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
